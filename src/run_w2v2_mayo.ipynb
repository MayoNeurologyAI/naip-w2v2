{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2vec 2.0\n",
    "\n",
    "We use w2v2 as implemented in [HuggingFace](https://huggingface.co/docs/transformers/model_doc/wav2vec2) and create wrapper classes for finetuning for our specific speech classification task as well as getting embeddings.\n",
    "\n",
    "Authors: Daniela Wiepert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, you will need access to google cloud storage bucket and the following packages must be installed on your system \n",
    "\n",
    "* albumentations (may run into issues in AIF)\n",
    "* librosa\n",
    "* torch, torchvision, torchaudio\n",
    "* tqdm\n",
    "\n",
    "(can ignore the following if using AIF)\n",
    "* google-cloud\n",
    "* google-cloud-storage\n",
    "* google-cloud-bigquery\n",
    "\n",
    "If working on a local computer, you can run the following commands to gain access to the google storage bucket\n",
    "\n",
    "```gcloud auth application-default login```\n",
    "\n",
    "```gcloud auth application-defaul set-quota-project PROJECT_NAME```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "#built-in\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#third-party\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "#local\n",
    "from utilities.dataloader_utils import *\n",
    "from models.w2v2_models import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "There are many mutable arguments when running w2v2. Please explore the different options and make sure all arguments are set as you would like. \n",
    "\n",
    "An important argument to consider is the model checkpoint. On AIF, it cannot load the model directly from HuggingFace. Instead, you should copy the following directory to your VM, and point the `-c` argument to it.\n",
    "\n",
    "```gsutil -m cp -r gs://gs://ml-e107-phi-shared-aif-us-p/m144443/checkpoints ./```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#google cloud storage\n",
    "parser.add_argument('-i','--prefix',default='speech_ai/speech_lake/', help='Input directory or location in google cloud storage bucket containing files to load')\n",
    "parser.add_argument(\"-s\", \"--study\", choices = ['r01_prelim','speech_poc_freeze_1', None], default='speech_poc_freeze_1', help=\"specify study name\")\n",
    "parser.add_argument(\"-d\", \"--data_split_root\", default='gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620', help=\"specify file path where datasplit is located. If you give a full file path to classification, an error will be thrown. On the other hand, evaluation and embedding expects a single .csv file.\")\n",
    "parser.add_argument('-l','--label_txt', default='mayo-w2v2/labels.txt')\n",
    "#GCS\n",
    "parser.add_argument('-b','--bucket_name', default='ml-e107-phi-shared-aif-us-p', help=\"google cloud storage bucket name\")\n",
    "parser.add_argument('-p','--project_name', default='ml-mps-aif-afdgpet01-p-6827', help='google cloud platform project name')\n",
    "#librosa vs torchaudio\n",
    "parser.add_argument('--lib', default=False, type=bool, help=\"Specify whether to load using librosa as compared to torch audio\")\n",
    "#output\n",
    "parser.add_argument(\"--dataset\", default='amr_subject_dedup_594_train_test_binarized_v2022062',type=str, help=\"the dataset used for training\")\n",
    "parser.add_argument(\"-o\", \"--exp_dir\", default=\"mayo-w2v2/experiments\")\n",
    "#Audio transforms\n",
    "parser.add_argument(\"--resample_rate\", default=16000,type=int, help='resample rate for audio files')\n",
    "parser.add_argument(\"--reduce\", default=True, type=bool, help=\"Specify whether to reduce to monochannel\")\n",
    "parser.add_argument(\"--clip_length\", default=160000, type=int, help=\"If truncating audio, specify clip length in # of frames. 0 = no truncation\")\n",
    "parser.add_argument(\"--trim\", default=True, type=int, help=\"trim silence\")\n",
    "#Mode specific\n",
    "parser.add_argument(\"-m\", \"--mode\", choices=['finetune','eval-only','extraction'], default='finetune')\n",
    "parser.add_argument(\"-mp\", \"--mdl_path\", default=None, help='If running eval-only or extraction, you have the option to load a fine-tuned model by specifying the save path here.')\n",
    "#Model parameters\n",
    "parser.add_argument(\"-c\", \"--checkpoint\", default=\"facebook/wav2vec2-base-960h\", help=\"specify path to pre-trained model weight checkpoint\")\n",
    "parser.add_argument(\"-n\", \"--num_labels\", type=int, default=6, help=\"specify number of features to classify\")\n",
    "parser.add_argument(\"-pm\", \"--pooling_mode\", default=\"mean\", help=\"specify method of pooling last hidden layer\", choices=['mean','sum','max'])\n",
    "parser.add_argument(\"-si\",\"--start\", type=int, default=16, help=\"specify starting column index where target label data can be located in data table\")\n",
    "parser.add_argument(\"-ei\", \"--end\", type=int, default=27, help=\"specify ending column index where target label data can be located in data table, starting index - ending index should encompass all target_labels columns\")\n",
    "parser.add_argument(\"-bs\", \"--batch_size\", type=int, default=8, help=\"specify batch size\")\n",
    "parser.add_argument(\"-nw\", \"--num_workers\", type=int, default=0, help=\"specify number of parallel jobs to run for data loader\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.0003, help=\"specify learning rate\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=1, help=\"specify number of training epochs\")\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"adam\"])\n",
    "parser.add_argument(\"--loss\", type=str, default=\"MSE\", help=\"the loss function for finetuning, depend on the task\", choices=[\"MSE\"])\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment\n",
    "The first step is to make sure the GCS bucket is initialized if given a `bucket_name`. Additionally, the list of target labels must be set. There are a few other arguments to consider as well\n",
    "\n",
    "In the original implementation, the list must be given as a `.txt` file to pass through the command line. In this implementation, we will set it as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda availability:  False\n"
     ]
    }
   ],
   "source": [
    "#CHECK GPU AVAILABLE\n",
    "print('Cuda availability: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m144443/miniconda3/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/m144443/miniconda3/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# (1) Set up GCS\n",
    "if args.bucket_name is not None:\n",
    "    storage_client = storage.Client(project=args.project_name)\n",
    "    bq_client = bigquery.Client(project=args.project_name)\n",
    "    bucket = storage_client.bucket(args.bucket_name)\n",
    "else:\n",
    "    bucket = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2), check if given study or if the prefix is the full prefix.\n",
    "if args.study is not None:\n",
    "    args.prefix = os.path.join(args.prefix, args.study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) get target labels\n",
    "    #get list of target labels\n",
    "args.target_labels = ['distorted Cs',\n",
    "                        'slow rate',\n",
    "                        'irregular artic breakdowns',\n",
    "                        'rapid rate',\n",
    "                        'distorted V',\n",
    "                        'strained']\n",
    "\n",
    "args.n_class = len(args.target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) check if output directory exists\n",
    "if not os.path.exists(args.exp_dir) and 'gs://' not in args.exp_dir:\n",
    "    os.mkdir(args.exp_dir)\n",
    "\n",
    "    # (5) check that clip length has been set\n",
    "    if args.clip_length == 0:\n",
    "        assert args.batch_size == 1, 'Not currently compatible with different length wave files unless batch size has been set to 1'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "We will start with the finetuning option and not include the option for only evaluating an already fine-tuned model (which is available in the full .py script)\n",
    "\n",
    "We load the data, set up simple WaveformDataset objets, and set up the dataloaders.\n",
    "\n",
    "The datasets and transforms are set up using functions from `utilities/dataloader_utils.py`. \n",
    "\n",
    "The dataloaders take in the datasets and batch size + number of workers.\n",
    "\n",
    "Please note that the resulting samples will be a dictionary with the keys `uid`, `waveform`, `targets`, `sample_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_traintest(args, bucket):\n",
    "    \"\"\"\n",
    "    Load the train and test data from a directory. Assumes the train and test data will exist in this directory under train.csv and test.csv\n",
    "    :param args: dict with all the argument values\n",
    "    :param bucket: google storage bucket object where data is saved\n",
    "    :return diag_train, diag_test: dataframes with target labels selected\n",
    "    \"\"\"\n",
    "    train_path = f'{args.data_split_root}/train.csv'\n",
    "    test_path = f'{args.data_split_root}/test.csv'\n",
    "    #get data\n",
    "    train_df = pd.read_csv(train_path, index_col = 'uid')\n",
    "    test_df = pd.read_csv(test_path, index_col = 'uid')\n",
    "\n",
    "    #get min number of columns containing all the target label columns\n",
    "    diag_train = train_df[args.target_labels]\n",
    "    diag_test = test_df[args.target_labels]\n",
    "    print(len(diag_test))\n",
    "    return diag_train, diag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(args, bucket):\n",
    "    \"\"\"\n",
    "    Set up pre-processing transform for raw samples \n",
    "    Loads data, reduces to 1 channel, downsamples, trims silence, truncate(?) and run feature extraction\n",
    "    :param args: dict with all the argument values\n",
    "    :param bucket: google storage bucket object where data is saved\n",
    "    return: transform: transforms object \n",
    "    \"\"\"\n",
    "    waveform_loader = UidToWaveform(prefix = args.prefix, bucket=bucket, lib=args.lib)\n",
    "    transform_list = [waveform_loader]\n",
    "    if args.reduce:\n",
    "        channel_sum = lambda w: torch.sum(w, axis = 0).unsqueeze(0)\n",
    "        mono_tfm = ToMonophonic(reduce_fn = channel_sum)\n",
    "        transform_list.append(mono_tfm)\n",
    "    if args.resample_rate != 0: #16000\n",
    "        downsample_tfm = Resample(args.resample_rate)\n",
    "        transform_list.append(downsample_tfm)\n",
    "    if args.trim:\n",
    "        trim_tfm = TrimSilence()\n",
    "        transform_list.append(trim_tfm)\n",
    "    if args.clip_length != 0: #160000\n",
    "        truncate_tfm = Truncate(length = args.clip_length)\n",
    "        transform_list.append(truncate_tfm)\n",
    "\n",
    "    tensor_tfm = ToTensor()\n",
    "    transform_list.append(tensor_tfm)\n",
    "    feature_tfm = Wav2VecFeatureExtractor(args.checkpoint)\n",
    "    transform_list.append(feature_tfm)\n",
    "    transform = torchvision.transforms.Compose(transform_list)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m144443/miniconda3/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# (1) load data\n",
    "assert '.csv' not in args.data_split_root, f'May have given a full file path, please confirm this is a directory: {args.data_split_root}'\n",
    "diag_train, diag_test = load_traintest(args, bucket) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) get data transforms    \n",
    "transform = get_transform(args, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) set up datasets and dataloaders\n",
    "dataset_train = WaveformDataset(diag_train, target_labels = args.target_labels, transform = transform)\n",
    "dataset_test = WaveformDataset(diag_test, target_labels = args.target_labels, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size = args.batch_size, shuffle = True, num_workers = args.num_workers)\n",
    "dataloader_test= DataLoader(dataset_test, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers)\n",
    "#dataloader_test = DataLoader(dataset_test, batch_size = len(diag_test), shuffle = False, num_workers = args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: TEST WHETHER YOU CAN LOAD A BATCH\n",
    "batch = next(iter(dataloader_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model\n",
    "\n",
    "Set up the model using classes from `w2v2_models.py`. This includes a wrapper for a speech classification model that adds on a classification head with a dense layer, ReLU, dropout, and a final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'projector.bias', 'wav2vec2.masked_spec_embed', 'classifier.weight', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# (4) initialize model\n",
    "model = Wav2Vec2ForSpeechClassification(args.checkpoint, args.pooling_mode, args.n_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training, evaluation, metrics\n",
    "\n",
    "Run the training loop, evaluate the test set, and get AUCs for the predictions from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(args, model, dataloader_train):\n",
    "    \"\"\"\n",
    "    Training loop for finetuning the w2v2 classification head. \n",
    "    :param args: dict with all the argument values\n",
    "    :param model: W2V2 model\n",
    "    :param dataloader_train: dataloader object with training data\n",
    "    :return model: fine-tuned w2v2 model\n",
    "    \"\"\"\n",
    "    print('Training start')\n",
    "    #send to gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    #loss\n",
    "    if args.loss == 'MSE':\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError('MSE must be given for loss parameter')\n",
    "    #optimizer\n",
    "    if args.optim == 'adam':\n",
    "        optim = torch.optim.Adam([p for p in model.parameters() if p.requires_grad],lr=args.learning_rate)\n",
    "    else:\n",
    "        raise ValueError('adam must be given for optimizer parameter')\n",
    "\n",
    "    #train\n",
    "    for e in range(args.epochs):\n",
    "        running_loss = 0\n",
    "        #t0 = time.time()\n",
    "        for batch in tqdm(dataloader_train):\n",
    "            x = torch.squeeze(batch['waveform'])\n",
    "            targets = batch['targets']\n",
    "            x, targets = x.to(device), targets.to(device)\n",
    "            optim.zero_grad()\n",
    "            o = model(x)\n",
    "            loss = criterion(o, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss_item = loss.item()\n",
    "            running_loss += loss_item\n",
    "\n",
    "        print('RUNNING LOSS', e, running_loss)\n",
    "\n",
    "    outname = \"_\".join(['w2v2_mdl', args.dataset, str(args.n_class), args.optim, str(args.epochs)+'epoch'])+'.pt'\n",
    "    outpath = os.path.join(args.exp_dir,outname)\n",
    "    torch.save(model.state_dict(), outpath)\n",
    "    torch.cuda.empty_cache()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(args, model, dataloader_eval):\n",
    "    \"\"\"\n",
    "    Start model evaluation\n",
    "    :param args: dict with all the argument values\n",
    "    :param model: W2V2 model\n",
    "    :param dataloader_eval: dataloader object with evaluation data\n",
    "    :return preds: model predictions\n",
    "    :return targets: model targets (actual values)\n",
    "    \"\"\"\n",
    "    print('Evaluation start')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    outputs = []\n",
    "    t = []\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(dataloader_eval):\n",
    "            x = torch.squeeze(batch['waveform'])\n",
    "            x = x.to(device)\n",
    "            targets = batch['targets']\n",
    "            targets = targets.to(device)\n",
    "            o = model(x)\n",
    "            outputs.append(o)\n",
    "            t.append(targets)\n",
    "    return outputs, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (6) start evaluating\n",
    "preds, targets = eval_loop(args, model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(args, preds, targets):\n",
    "    \"\"\"\n",
    "    Get AUC scores, doesn't return, just saves the metrics to a csv\n",
    "    :param args: dict with all the argument values\n",
    "    :param preds: model predictions\n",
    "    :param targets: model targets (actual values)\n",
    "    \"\"\"\n",
    "    #get AUC score and all data for ROC curve\n",
    "    metrics = {}\n",
    "    pred_mat=torch.sigmoid(torch.cat(preds)).cpu().detach().numpy()\n",
    "    target_mat=torch.cat(targets).cpu().detach().numpy()\n",
    "    aucs=roc_auc_score(target_mat, pred_mat, average = None)\n",
    "    print(aucs)\n",
    "    data = pd.DataFrame({'Label':args.target_labels, 'AUC':aucs})\n",
    "    data.to_csv(os.path.join(args.exp_dir, 'metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) performance metrics\n",
    "metrics(args, preds, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings\n",
    "\n",
    "Embedding extraction is a slightly different process. We instead load in one csv file, initialize and load a finetuned model, then run the embedding loop which extracts the last hidden layer (which functions as the embedding of dim 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.data_split_root = 'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620/test.csv'\n",
    "args.mode = 'extraction'\n",
    "args.mdl_path = None #TODO: must set to a finetuned model if you want it to load and get embeddings in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(args, bucket):\n",
    "    df = pd.read_csv(args.data_split_root, index_col = 'uid')\n",
    "    diag_df = df[args.target_labels]\n",
    "    return diag_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for embeddings, the code expects `args.data_split_root` to be a csv rather than a directory,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_split_root = 'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) load data to get embeddings for\n",
    "assert '.csv' in args.data_split_root, f'A csv file is necessary for embedding extraction. Please make sure this is a full file path: {args.data_split_root}'\n",
    "annotations_df = load_csv(args, bucket)\n",
    "\n",
    "# (2) get transforms\n",
    "transform = get_transform(args, bucket)\n",
    "\n",
    "# (3) set up dataloaders\n",
    "waveform_dataset = WaveformDataset(annotations_df = annotations_df, target_labels = args.target_labels, transform = transform)\n",
    "dataloader = DataLoader(waveform_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the embedding model requires a checkpoint to a pretrained model path, a pooling mode selection, and a path to a finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (4) set up embedding model\n",
    "model = Wav2Vec2ForEmbeddingExtraction(args.checkpoint, args.pooling_mode, args.mdl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_loop(model, dataloader):\n",
    "    \"\"\"\n",
    "    Run a specific subtype of evaluation for getting embeddings.\n",
    "    :param args: dict with all the argument values\n",
    "    :param model: W2V2 model\n",
    "    :param dataloader_eval: dataloader object with data to get embeddings for\n",
    "    :return embeddings: an np array containing the embeddings\n",
    "    \"\"\"\n",
    "    print('Getting embeddings')\n",
    "    embeddings = np.array([])\n",
    "\n",
    "    # send to gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(dataloader):\n",
    "            x = torch.squeeze(batch['waveform'])\n",
    "            x = x.to(device)\n",
    "            e = model(x)\n",
    "            e = e.cpu().numpy()\n",
    "            if embeddings.size == 0:\n",
    "                embeddings = e\n",
    "            else:\n",
    "                embeddings = np.append(embeddings, e, axis=0)\n",
    "        \n",
    "    print(embeddings.shape)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) get embeddings\n",
    "embeddings = embedding_loop(model, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.DataFrame([[r] for r in embeddings], columns = ['embedding'], index=annotations_df.index)\n",
    "\n",
    "outname = \"_\".join([args.dataset, 'w2v2_embeddings'])+'.pqt'\n",
    "outpath = os.path.join(args.exp_dir,outname)\n",
    "\n",
    "df_embed.to_parquet(outpath) #TODO: fix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
