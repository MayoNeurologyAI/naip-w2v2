{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2vec 2.0\n",
    "\n",
    "We use w2v2 as implemented in [HuggingFace](https://huggingface.co/docs/transformers/model_doc/wav2vec2) and create wrapper classes for finetuning for our specific speech classification task as well as getting embeddings.\n",
    "\n",
    "Note: [run.py](https://github.com/dwiepert/mayo-w2v2/blob/main/src/run.py) script can also do evaluation only. \n",
    "\n",
    "Authors: Daniela Wiepert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, you will need access to google cloud storage bucket and the following packages must be installed on your system \n",
    "\n",
    "* albumentations (may run into issues in AIF)\n",
    "* librosa\n",
    "* torch, torchvision, torchaudio\n",
    "* tqdm\n",
    "* transformers\n",
    "* pyarrow\n",
    "\n",
    "(can ignore the following if using AIF)\n",
    "* google-cloud-storage\n",
    "\n",
    "The [requirements.txt](https://github.com/dwiepert/mayo-w2v2/blob/main/requirements.txt) can be used to set up this environment. \n",
    "\n",
    "To access data stored in GCS on your local machine, you will need to additionally run\n",
    "\n",
    "```gcloud auth application-default login```\n",
    "\n",
    "```gcloud auth application-defaul set-quota-project PROJECT_NAME```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "#built-in\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#third-party\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "from google.cloud import storage\n",
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "#local\n",
    "from utilities import *\n",
    "from models import *\n",
    "#from loops import *\n",
    "from dataloader import W2V2Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload/Download functions & Data loading functions\n",
    "These are defined in `utilities/load_utils.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "There are many mutable arguments when running w2v2. Please explore the different options and make sure all arguments are set as you would like. \n",
    "\n",
    "An important argument to consider is the model checkpoint. On AIF, it cannot load the model directly from HuggingFace. Instead, you can look at `gs://ml-e107-phi-shared-aif-us-p/m144443/checkpoints` for options. The default is `gs://ml-e107-phi-shared-aif-us-p/m144443/checkpoints/wav2vec2-base-960h`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of arguments, specifically note the `weighted` and `layer` arguments which alter the training functionality.\n",
    "\n",
    "`weighted` initializes another set of parameters to learn, wherein all hidden states have learned weights to indicate the contribution of the layers to classification. \n",
    "\n",
    "`layer` sets which hidden state to use as input to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#Inputs\n",
    "parser.add_argument('-i','--prefix',default='speech_ai/speech_lake/', help='Input directory or location in google cloud storage bucket containing files to load')\n",
    "parser.add_argument(\"-s\", \"--study\", choices = ['r01_prelim','speech_poc_freeze_1', None], default='speech_poc_freeze_1', help=\"specify study name\")\n",
    "parser.add_argument(\"-d\", \"--data_split_root\", default='gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620/test.csv', help=\"specify file path where datasplit is located. If you give a full file path to classification, an error will be thrown. On the other hand, evaluation and embedding expects a single .csv file.\")\n",
    "parser.add_argument('-l','--label_txt', default='./labels.txt')\n",
    "parser.add_argument('--lib', default=False, type=bool, help=\"Specify whether to load using librosa as compared to torch audio\")\n",
    "parser.add_argument(\"-c\", \"--checkpoint\", default=\"gs://ml-e107-phi-shared-aif-us-p/m144443/checkpoints/wav2vec2-base-960h\", help=\"specify path to pre-trained model weight checkpoint\")\n",
    "parser.add_argument(\"-mp\", \"--finetuned_mdl_path\", default='/Users/m144443/Documents/GitHub/mayo-w2v2/experiments/weighted/amr_subject_dedup_594_train_100_test_binarized_v20220620_5_adam_epoch1_wav2vec2-base-960h_mdl_weighted.pt', help='If running eval-only or extraction, you have the option to load a fine-tuned model by specifying the save path here. If passed a gs:// file, will download to local machine.')\n",
    "#GCS\n",
    "parser.add_argument('-b','--bucket_name', default='ml-e107-phi-shared-aif-us-p', help=\"google cloud storage bucket name\")\n",
    "parser.add_argument('-p','--project_name', default='ml-mps-aif-afdgpet01-p-6827', help='google cloud platform project name')\n",
    "parser.add_argument('--cloud', default=False, type=bool, help=\"Specify whether to save everything to cloud\")\n",
    "#output\n",
    "parser.add_argument(\"--dataset\", default=None,type=str, help=\"When saving, the dataset arg is used to set file names. If you do not specify, it will assume the lowest directory from data_split_root\")\n",
    "parser.add_argument(\"-o\", \"--exp_dir\", default=\"./experiments/embeddings\", help='specify LOCAL output directory')\n",
    "parser.add_argument('--cloud_dir', default='m144443/temp_out/w2v2_ft_weighted', type=str, help=\"if saving to the cloud, you can specify a specific place to save to in the CLOUD bucket\")\n",
    "#Mode specific\n",
    "parser.add_argument(\"--weighted\", type=bool, default=True, help=\"specify whether to learn a weighted sum of layers for classification\")\n",
    "parser.add_argument(\"--layer\", default=-1, type=int, help=\"specify which hidden state is being used. It can be between -1 and 12\")\n",
    "parser.add_argument(\"--freeze\", type=bool, default=True, help='specify whether to freeze the base model')\n",
    "parser.add_argument('--embedding_type', type=str, default='wt', help='specify whether embeddings should be extracted from classification head (ft) or base pretrained model (pt)', choices=['ft','pt'])\n",
    "#Audio transforms\n",
    "parser.add_argument(\"--resample_rate\", default=16000,type=int, help='resample rate for audio files')\n",
    "parser.add_argument(\"--reduce\", default=True, type=bool, help=\"Specify whether to reduce to monochannel\")\n",
    "parser.add_argument(\"--clip_length\", default=160000, type=int, help=\"If truncating audio, specify clip length in # of frames. 0 = no truncation\")\n",
    "parser.add_argument(\"--trim\", default=False, type=int, help=\"trim silence\")\n",
    "#Model parameters\n",
    "parser.add_argument(\"-pm\", \"--pooling_mode\", default=\"mean\", help=\"specify method of pooling last hidden layer\", choices=['mean','sum','max'])\n",
    "parser.add_argument(\"-bs\", \"--batch_size\", type=int, default=8, help=\"specify batch size\")\n",
    "parser.add_argument(\"-nw\", \"--num_workers\", type=int, default=0, help=\"specify number of parallel jobs to run for data loader\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.0003, help=\"specify learning rate\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=1, help=\"specify number of training epochs\")\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"adam\"])\n",
    "parser.add_argument(\"--loss\", type=str, default=\"BCE\", help=\"the loss function for finetuning, depend on the task\", choices=[\"MSE\", \"BCE\"])\n",
    "parser.add_argument(\"--scheduler\", type=str, default=None, help=\"specify lr scheduler\", choices=[\"onecycle\", None])\n",
    "parser.add_argument(\"--max_lr\", type=float, default=0.01, help=\"specify max lr for lr scheduler\")\n",
    "#classification head parameters\n",
    "parser.add_argument(\"--activation\", type=str, default='relu', help=\"specify activation function to use for classification head\")\n",
    "parser.add_argument(\"--final_dropout\", type=float, default=0.25, help=\"specify dropout probability for final dropout layer in classification head\")\n",
    "parser.add_argument(\"--layernorm\", type=bool, default=False, help=\"specify whether to include the LayerNorm in classification head\")\n",
    "#OTHER\n",
    "parser.add_argument(\"--debug\", default=True, type=bool)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment\n",
    "The first step is to make sure the GCS bucket is initialized if given a `bucket_name`. Additionally, the list of target labels must be set. There are a few other arguments to consider as well\n",
    "\n",
    "In the original implementation, the list must be given as a `.txt` file to pass through the command line. In this implementation, we will set it as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda availability:  False\n"
     ]
    }
   ],
   "source": [
    "#CHECK GPU AVAILABLE\n",
    "print('Cuda availability: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m144443/miniconda3/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/m144443/miniconda3/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# (1) Set up GCS\n",
    "if args.bucket_name is not None:\n",
    "    storage_client = storage.Client(project=args.project_name)\n",
    "    bq_client = bigquery.Client(project=args.project_name)\n",
    "    bucket = storage_client.bucket(args.bucket_name)\n",
    "else:\n",
    "    bucket = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2), check if given study or if the prefix is the full prefix.\n",
    "if args.study is not None:\n",
    "    args.prefix = os.path.join(args.prefix, args.study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) get dataset name\n",
    "if args.dataset is None:\n",
    "    if '.csv' in args.data_split_root:\n",
    "        args.dataset = '{}_{}'.format(os.path.basename(os.path.dirname(args.data_split_root)), os.path.basename(args.data_split_root[:-4]))\n",
    "    else:\n",
    "        args.dataset = os.path.basename(args.data_split_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) get target labels\n",
    "    #get list of target labels\n",
    "args.target_labels = ['slow rate',\n",
    "                        'irregular artic breakdowns',\n",
    "                        'rapid rate',\n",
    "                        'distortions',\n",
    "                        'strained']\n",
    "\n",
    "args.n_class = len(args.target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (5) check if output directory exists, SHOULD NOT BE A GS:// path\n",
    "if not os.path.exists(args.exp_dir):\n",
    "    os.makedirs(args.exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) check that clip length has been set\n",
    "if args.clip_length == 0:\n",
    "    try: \n",
    "        assert args.batch_size == 1, 'Not currently compatible with different length wav files unless batch size has been set to 1'\n",
    "    except:\n",
    "        args.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) dump arguments\n",
    "args_path = \"%s/args.pkl\" % args.exp_dir\n",
    "with open(args_path, \"wb\") as f:\n",
    "    pickle.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) check if checkpoint is stored in gcs bucket or confirm it exists on local machine\n",
    "assert args.checkpoint is not None, 'Must give a model checkpoint for W2V2'\n",
    "args.checkpoint = gcs_model_exists(args.checkpoint, args.bucket_name, args.exp_dir, bucket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) dump arguments\n",
    "args_path = \"%s/args.pkl\" % args.exp_dir\n",
    "with open(args_path, \"wb\") as f:\n",
    "    pickle.dump(args, f)\n",
    "#in case of error, everything is immediately uploaded to the bucket\n",
    "if args.cloud:\n",
    "    upload(args.cloud_dir, args_path, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(9) add bucket to args\n",
    "args.bucket = bucket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "We will start with the finetuning option and not include the option for only evaluating an already fine-tuned model (which is available in the full .py script)\n",
    "\n",
    "When loading data, we start with a data split root, which we expect to be a directory containing a `train.csv` file and `test.csv` file with file names for train/test and the associated label data.\n",
    "\n",
    "We load the data, set up an audio configuration, set up `W2V2Dataset` objects (from `dataloader.py`), and set up the dataloaders.\n",
    "\n",
    "The transforms in `W2V2Dataset` are set up using functions from `utilities/speech_utils.py`. \n",
    "\n",
    "The dataloaders take in the datasets and batch size + number of workers.\n",
    "\n",
    "Please note that the resulting samples will be a dictionary with the keys `uid`, `waveform`, `targets`, `sample_rate`.\n",
    "\n",
    "We randomly sample the train.csv within the load_data function to get a validation dataset of 50 samples. See `load_utils.py` for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m144443/miniconda3/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print('Running finetuning: ')\n",
    "# (1) load data\n",
    "assert '.csv' not in args.data_split_root, f'May have given a full file path, please confirm this is a directory: {args.data_split_root}'\n",
    "train_df, val_df, test_df = load_data(args.data_split_root, args.target_labels, args.exp_dir, args.cloud, args.cloud_dir, args.bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) set up audio configuration for transforms\n",
    "audio_conf = {'checkpoint': args.checkpoint, 'resample_rate':args.resample_rate, 'reduce': args.reduce,\n",
    "                'trim': args.trim, 'clip_length': args.clip_length}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (3) set up datasets and dataloaders\n",
    "dataset_train = W2V2Dataset(train_df, target_labels = args.target_labels,\n",
    "                            audio_conf = audio_conf, prefix=args.prefix, bucket=args.bucket, librosa=args.lib)\n",
    "dataset_val = W2V2Dataset(val_df, target_labels = args.target_labels,\n",
    "                            audio_conf = audio_conf, prefix=args.prefix, bucket=args.bucket, librosa=args.lib)\n",
    "dataset_test = W2V2Dataset(test_df, target_labels = args.target_labels,\n",
    "                            audio_conf = audio_conf, prefix=args.prefix, bucket=args.bucket, librosa=args.lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size = args.batch_size, shuffle = True, num_workers = args.num_workers)\n",
    "dataloader_val= DataLoader(dataset_val, batch_size = 1, shuffle = False, num_workers = args.num_workers)\n",
    "dataloader_test= DataLoader(dataset_test, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: TEST WHETHER YOU CAN LOAD A BATCH\n",
    "batch = next(iter(dataloader_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model\n",
    "\n",
    "Set up the model using classes from `w2v2_models.py`. This includes a wrapper for a speech classification model that adds on a classification head with a dense layer, ReLU, dropout, and a final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'projector.bias', 'wav2vec2.masked_spec_embed', 'classifier.weight', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# (4) initialize model\n",
    "model = Wav2Vec2ForSpeechClassification(checkpoint=args.checkpoint, num_labels = args.n_class, pooling_mode = args.pooling_mode, \n",
    "                                        freeze=args.freeze, activation=args.activation, final_dropout=args.final_dropout, \n",
    "                                        layernorm=args.layernorm, weighted=args.weighted, layer=args.layer)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training, evaluation\n",
    "\n",
    "The model training loops are originally implemented in `loops.py`, but we will include them here for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, dataloader_val):\n",
    "    '''\n",
    "    Validation loop for finetuning the w2v2 classification head. \n",
    "    :param model: W2V2 model\n",
    "    :param criterion: loss function\n",
    "    :param dataloader_val: dataloader object with validation data\n",
    "    :return validation_loss: list with validation loss for each batch\n",
    "    '''\n",
    "    validation_loss = list()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(dataloader_val):\n",
    "            x = torch.squeeze(batch['waveform'], dim=1)\n",
    "            targets = batch['targets']\n",
    "            x, targets = x.to(device), targets.to(device)\n",
    "            o = model(x)\n",
    "            val_loss = criterion(o, targets)\n",
    "            validation_loss.append(val_loss.item())\n",
    "\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, dataloader_train, dataloader_val = None, \n",
    "             optim='adamw', learning_rate=0.001, loss_fn='BCE',\n",
    "             sched='onecycle', max_lr=0.01,\n",
    "             epochs=10, exp_dir='', cloud=False, cloud_dir='', bucket=None):\n",
    "    \"\"\"\n",
    "    Training loop for finetuning W2V2\n",
    "    :param model: W2V2 model\n",
    "    :param dataloader_train: dataloader object with training data\n",
    "    :param dataloader_val: dataloader object with validation data\n",
    "    :param optim: type of optimizer to initialize\n",
    "    :param learning_rate: optimizer learning rate\n",
    "    :param loss_fn: type of loss function to initialize\n",
    "    :param sched: type of scheduler to initialize\n",
    "    :param max_lr: max learning rate for onecycle scheduler\n",
    "    :param epochs: number of epochs to run pretraining\n",
    "    :param exp_dir: output directory on local machine\n",
    "    :param cloud: boolean indicating whether uploading to cloud\n",
    "    :param cloud_dir: output directory in google cloud storage bucket\n",
    "    :param bucket: initialized GCS bucket object\n",
    "    :return model: finetuned W2V2 model\n",
    "    \"\"\"\n",
    "    print('Training start')\n",
    "    #send to gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    #loss\n",
    "    if loss_fn == 'MSE':\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    elif loss_fn == 'BCE':\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError('MSE must be given for loss parameter')\n",
    "    #optimizer\n",
    "    if optim == 'adam':\n",
    "        optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad],lr=learning_rate)\n",
    "    elif optim == 'adamw':\n",
    "         optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError('adam must be given for optimizer parameter')\n",
    "    \n",
    "    if sched == 'onecycle':\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=max_lr, steps_per_epoch=len(dataloader_train), epochs=epochs)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    #train\n",
    "    for e in range(epochs):\n",
    "        training_loss = list()\n",
    "        #t0 = time.time()\n",
    "        model.train()\n",
    "        for batch in tqdm(dataloader_train):\n",
    "            x = torch.squeeze(batch['waveform'], dim=1)\n",
    "            targets = batch['targets']\n",
    "            x, targets = x.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            o = model(x)\n",
    "            loss = criterion(o, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            loss_item = loss.item()\n",
    "            training_loss.append(loss_item)\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            #SET UP LOGS\n",
    "            if scheduler is not None:\n",
    "                lr = scheduler.get_last_lr()\n",
    "            else:\n",
    "                lr = learning_rate\n",
    "            logs = {'epoch': e, 'optim':optim, 'loss_fn': loss_fn, 'lr': lr, 'scheduler':sched}\n",
    "    \n",
    "            logs['training_loss_list'] = training_loss\n",
    "            training_loss = np.array(training_loss)\n",
    "            logs['running_loss'] = np.sum(training_loss)\n",
    "            logs['training_loss'] = np.mean(training_loss)\n",
    "\n",
    "            print('RUNNING LOSS', e, np.sum(training_loss) )\n",
    "            print(f'Training loss: {np.mean(training_loss)}')\n",
    "\n",
    "            if dataloader_val is not None:\n",
    "                print(\"Validation start\")\n",
    "                validation_loss = validation(model, criterion, dataloader_val)\n",
    "\n",
    "                logs['val_loss_list'] = validation_loss\n",
    "                validation_loss = np.array(validation_loss)\n",
    "                logs['val_running_loss'] = np.sum(validation_loss)\n",
    "                logs['val_loss'] = np.mean(validation_loss)\n",
    "                \n",
    "                print('RUNNING VALIDATION LOSS',e, np.sum(validation_loss) )\n",
    "                print(f'Validation loss: {np.mean(validation_loss)}')\n",
    "            \n",
    "            #SAVE LOGS\n",
    "            print(f'Saving epoch {e}')\n",
    "            json_string = json.dumps(logs)\n",
    "            logs_path = os.path.join(exp_dir, 'logs_ft_epoch{}.json'.format(e))\n",
    "            with open(logs_path, 'w') as outfile:\n",
    "                json.dump(json_string, outfile)\n",
    "            \n",
    "            #SAVE CURRENT MODEL\n",
    "            \n",
    "            mdl_path = os.path.join(exp_dir, 'w2v2_ft_mdl_epoch{}.pt'.format(e))\n",
    "            torch.save(model.state_dict(), mdl_path)\n",
    "            \n",
    "            optim_path = os.path.join(exp_dir, 'w2v2_ft_optim_epoch{}.pt'.format(e))\n",
    "            torch.save(optimizer.state_dict(), optim_path)\n",
    "\n",
    "            if cloud:\n",
    "                upload(cloud_dir, logs_path, bucket)\n",
    "                #upload_from_memory(model.state_dict(), args.cloud_dir, mdl_path, args.bucket)\n",
    "                upload(cloud_dir, mdl_path, bucket)\n",
    "                upload(cloud_dir, optim_path, bucket)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader_eval, exp_dir, cloud=False, cloud_dir=None, bucket=None):\n",
    "    \"\"\"\n",
    "    Start model evaluation\n",
    "    :param model: W2V2 model\n",
    "    :param dataloader_eval: dataloader object with evaluation data\n",
    "    :param exp_dir: specify LOCAL output directory as str\n",
    "    :param cloud: boolean to specify whether to save everything to google cloud storage\n",
    "    :param cloud_dir: if saving to the cloud, you can specify a specific place to save to in the CLOUD bucket\n",
    "    :param bucket: google cloud storage bucket object\n",
    "    :return preds: model predictions\n",
    "    :return targets: model targets (actual values)\n",
    "    \"\"\"\n",
    "    print('Evaluation start')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    outputs = []\n",
    "    t = []\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(dataloader_eval):\n",
    "            x = torch.squeeze(batch['waveform'], dim=1)\n",
    "            x = x.to(device)\n",
    "            targets = batch['targets']\n",
    "            targets = targets.to(device)\n",
    "            o = model(x)\n",
    "            outputs.append(o)\n",
    "            t.append(targets)\n",
    "\n",
    "    outputs = torch.cat(outputs).cpu().detach()\n",
    "    t = torch.cat(t).cpu().detach()\n",
    "    # SAVE PREDICTIONS AND TARGETS \n",
    "    print('Saving predictions')\n",
    "    pred_path = os.path.join(exp_dir, 'w2v2_eval_predictions.pt')\n",
    "    target_path = os.path.join(exp_dir, 'w2v2_eval_targets.pt')\n",
    "    torch.save(outputs, pred_path)\n",
    "    torch.save(t, target_path)\n",
    "\n",
    "    if cloud:\n",
    "        upload(cloud_dir, pred_path, bucket)\n",
    "        upload(cloud_dir, target_path, bucket)\n",
    "\n",
    "    print('Evaluation finished')\n",
    "    return outputs, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (5) start fine-tuning classification\n",
    "model = finetune(model, dataloader_train, dataloader_val,\n",
    "                    args.optim, args.learning_rate, args.loss, \n",
    "                    args.scheduler, args.max_lr, args.epochs,\n",
    "                    args.exp_dir, args.cloud, args.cloud_dir, args.bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving final epoch')\n",
    "\n",
    "if model.weighted:\n",
    "    mdl_path = os.path.join(args.exp_dir, '{}_{}_{}_epoch{}_{}_mdl_weighted.pt'.format(args.dataset, args.n_class, args.optim, args.epochs, os.path.basename(args.checkpoint)))\n",
    "else:\n",
    "    if args.layer==-1:\n",
    "        args.layer='Final'\n",
    "    mdl_path = os.path.join(args.exp_dir, '{}_{}_{}_layer{}_epoch{}_{}_mdl.pt'.format(args.dataset, args.n_class, args.optim, args.layer, args.epochs, os.path.basename(args.checkpoint)))\n",
    "torch.save(model.state_dict(), mdl_path)\n",
    "\n",
    "if args.cloud:\n",
    "    upload(args.cloud_dir, mdl_path, args.bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) start evaluating\n",
    "preds, targets = evaluation(model, dataloader_test, args.exp_dir, args.cloud, args.cloud_dir, args.bucket)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings\n",
    "\n",
    "Embedding extraction is a slightly different process. We instead load in one csv file, initialize and load a finetuned model, then run the embedding loop which extracts a hidden layer if specified as 'pt', the weighted sum output if specified as 'wt', or the output of the first layer of the classification head 'ft' (which functions as the embedding of dim 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.data_split_root = 'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620/test.csv'\n",
    "args.embedding_type='ft' #if 'pt', it will get embeddings from only the pretrained model, 'wt' from weighted sum parameter\n",
    "#args.mdl_path = None #TODO: must set to a finetuned model if you want it to load and get embeddings in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original model arguments\n",
    "model_args, args.finetuned_mdl_path = setup_mdl_args(args, args.finetuned_mdl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running Embedding Extraction: ')\n",
    "# (1) load data to get embeddings for\n",
    "assert '.csv' in args.data_split_root, f'A csv file is necessary for embedding extraction. Please make sure this is a full file path: {args.data_split_root}'\n",
    "annotations_df = pd.read_csv(args.data_split_root, index_col = 'uid') #data_split_root should use the CURRENT arguments regardless of the finetuned model\n",
    "annotations_df[\"distortions\"]=((annotations_df[\"distorted Cs\"]+annotations_df[\"distorted V\"])>0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) set up audio configuration for transforms\n",
    "audio_conf = {'checkpoint': args.checkpoint, 'resample_rate':args.resample_rate, 'reduce': args.reduce,\n",
    "                'trim': args.trim, 'clip_length': args.clip_length}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) set up dataloaders\n",
    "waveform_dataset = W2V2Dataset(annotations_df = annotations_df, target_labels = model_args.target_labels,\n",
    "                                audio_conf = audio_conf, prefix=args.prefix, bucket=args.bucket, librosa=args.lib) #not super important for embeddings, but the dataset should be selecting targets based on the FINETUNED model\n",
    "\n",
    "dataloader = DataLoader(waveform_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) set up embedding model\n",
    "model = Wav2Vec2ForSpeechClassification(checkpoint=model_args.checkpoint, num_labels = model_args.n_class, pooling_mode = model_args.pooling_mode, \n",
    "                                        freeze=model_args.freeze, activation=model_args.activation, final_dropout=model_args.final_dropout, \n",
    "                                        layernorm=model_args.layernorm, weighted=model_args.weighted, layer=model_args.layer)   #should look like the finetuned model (so using model_args). If pretrained model, will resort to current args\n",
    "\n",
    "if args.finetuned_mdl_path is not None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sd = torch.load(args.finetuned_mdl_path, map_location=device)\n",
    "    model.load_state_dict(sd, strict=False)\n",
    "else:\n",
    "    print(f'Extracting embeddings from only a pretrained model: {args.pretrained_mdl_path}. Extraction method changed to pt.')\n",
    "    args.embedding_type = 'pt' #manually change the type to 'pt' if not given a finetuned mdl path.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the embedding extraction loop is implemented in `loops.py`, but we will include it here for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_extraction(model, dataloader,embedding_type='ft',layer=-1, pooling_mode='mean'):\n",
    "    \"\"\"\n",
    "    Run a specific subtype of evaluation for getting embeddings.\n",
    "    :param model: W2V2 model\n",
    "    :param dataloader_eval: dataloader object with data to get embeddings for\n",
    "    :param embedding_type: string specifying whether embeddings should be extracted from classification head (ft) or base pretrained model (pt)\n",
    "    :return embeddings: an np array containing the embeddings\n",
    "    :param layer: hidden layer to take out and do results for - must be between 0-12\n",
    "    \"\"\"\n",
    "    print('Getting embeddings')\n",
    "    embeddings = np.array([])\n",
    "\n",
    "    # send to gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(dataloader):\n",
    "            x = torch.squeeze(batch['waveform'], dim=1)\n",
    "            x = x.to(device)\n",
    "            e = model.extract_embedding(x, embedding_type,layer=layer, pooling_mode=pooling_mode)\n",
    "            e = e.cpu().numpy()\n",
    "            if embeddings.size == 0:\n",
    "                embeddings = e\n",
    "            else:\n",
    "                embeddings = np.append(embeddings, e, axis=0)\n",
    "        \n",
    "    return embeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note all the different arguments for the embedding_extraction code. \n",
    "You can change the embedding type to:\n",
    "* `ft`: extracts the embedding from the dense layer of the classification head\n",
    "* `pt`: extracts the embedding from a hidden state (specified with `layer`)\n",
    "* `wt`: extracts the embedding after weighted sum of layers\n",
    "\n",
    "You can also change the pooling mode to one of 'mean', 'sum', 'max'.\n",
    "\n",
    "The main embedding extraction code is implemented in the `w2v2_models.py` `W2V2ForSpeechClassification` class under `extract_embedding(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) get embeddings\n",
    "embeddings = embedding_extraction(model, dataloader, args.embedding_type, \n",
    "                                  args.layer, args.pooling_mode)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.DataFrame([[r] for r in embeddings], columns = ['embedding'], index=annotations_df.index)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embedding_type == 'ft':\n",
    "    args.layer='NA'\n",
    "    args.pooling_mode='NA'\n",
    "elif args.embedding_type == 'wt':\n",
    "    args.layer='NA'\n",
    "elif args.layer==-1:\n",
    "    args.layer='Final'\n",
    "\n",
    "try:\n",
    "    pqt_path = '{}/{}_layer{}_{}_w2v2_{}_embeddings.pqt'.format(args.exp_dir, args.dataset, args.layer, args.pooling_mode,args.embedding_type)\n",
    "    \n",
    "    df_embed.to_parquet(path=pqt_path, index=True, engine='pyarrow') #TODO: fix\n",
    "\n",
    "    if args.cloud:\n",
    "        upload(args.cloud_dir, pqt_path, args.bucket)\n",
    "except:\n",
    "    print('Unable to save as pqt, saving instead as csv')\n",
    "    csv_path = '{}/{}_layer{}_{}_w2v2_{}_embeddings.csv'.format(args.exp_dir, args.dataset, args.layer, args.pooling_mode,args.embedding_type)\n",
    "    \n",
    "    df_embed.to_csv(csv_path, index=True)\n",
    "\n",
    "    if args.cloud:\n",
    "        upload(args.cloud_dir, csv_path, args.bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
